<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="author" content="Shiyao Xu">
  <meta name="description" content="Shiyao Xu's Homepage">
  <meta name="keywords" content="Shiyao Xu,ÂæêËØóÁë∂,homepage,‰∏ªÈ°µ,computer vision,Peking University,Alibaba,image generation,texture synthesis,3D reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Shiyao Xu (ÂæêËØóÁë∂)'s Homepage</title>
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shiyao Xu (ÂæêËØóÁë∂)</name>
              </p>
              <p style="text-align:center">
                Email: xusy[at]stu.pku.edu.cn/xsy9915[at]gmail.com &nbsp;&nbsp;<a href="https://github.com/41xu">Github</a>&nbsp;&nbsp;<a href="paper/Shiyao_Xu_CV_Nov_22.pdf">CV</a>&nbsp;&nbsp;<a href="https://scholar.google.com/citations?user=uK8FvBQAAAAJ">Google Scholar</a>&nbsp;&nbsp;<a href="https://twitter.com/xusy2333">Twitter</a>&nbsp;&nbsp;<a href="https://www.zhihu.com/people/zhuo-ji-dui-chang">Áü•‰πé</a>
              </p>
              <p>
                I am Shiyao Xu (ÂæêËØóÁë∂), currently a final year master student in <a href="https://www.icst.pku.edu.cn/">Wangxuan Institute of Computer Techonology(WICT)</a> at <a href="https://www.pku.edu.cn/">Peking University</a>, China, supervised by <a href="https://www.wict.pku.edu.cn/zlian/">Zhouhui Lian</a>. 
                I'm a member of <a href="http://59.108.48.27/cscl/">CSCL group</a> led by <a href="https://www.wict.pku.edu.cn/zlian/">Prof. Zhouhui Lian</a>, and also a research intern in <a href="https://damo.alibaba.com/labs/xr-lab">XR-Lab</a> at <a href="https://damo.alibaba.com/">DAMO Academy</a>, Alibaba Group. 
                I'm now fortunately supervised by <a href="https://lingzhili.com/">Lingzhi Li</a> and <a href="https://lishen-shirley.github.io/">Dr. Li Shen</a> in Alibaba. I did my bachelors at <a href="https://www.dlut.edu.cn/">Dalian University of Technology</a>, China, 2020. 
              
              </p>
              <!-- <p>
                I'm now an incoming Ph.D. in <a href="https://team.inria.fr/mimetic/">MimeTIC research team</a> at <a href="https://www.inria.fr/en">Inria</a>, France, working with <a href="https://boukhayma.github.io/">Adnane Boukhayma</a> and <a href="https://is.mpg.de/~vabrevaya">Victoria Fern√°ndez Abrevaya</a>. 
              </p> -->
              <p>
                <strong><font color=red>I am looking for a PhD position or full-time, -paid research assistant opportunity starting from now.</font></strong> Please freely contact me! :)
              </p>
          
              <p>
                My research interests lie in neural rendering and generation models.
              </p>
            </td>
            <td style="padding:15% 7% 7% 7%;width:40%;max-width:40%">
              <a href="images/Shiyao_Xu_Cartoon.PNG"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Shiyao_Xu_Cartoon.PNG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News!üî•</heading>
            <li><font color=red>[News!]</font>2023.04: FINALLY! Our paper: <i>DeSRF: DeformableStylized Radiance Field</i> is accepted by CVPR 2023 Workshop: <a href="https://generative-vision.github.io/workshop-CVPR-23/">Generative Models for Computer Vision</a>. </li>
            <li><font color=red>[News!]</font>2022.10: Recieve a graduate school scholarship!</li>
            <li>2022.08: Happy to announce that our paper "Your3dEmoji"" is accepted by <strong>SIGGRAPH ASIA 2022</strong> Tech. Comm.!ü§™ See you in Korea!</li>
            <li>2022.08: Serve as a Student Volunteer (virtual) in <strong>SIGGRAPH 2022</strong>.</li>
            <li>‚öΩÔ∏è A member of PKU Women Football Club. Also a founder of our college women's football team.(I'm also a fan of Arsenal F.C.üî´üî¥‚ö™Ô∏è)</li>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications and Preprints</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                  <source src='images/3-teaser.mov'>
                </video>              
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>DeSRF: Deformable Stylized Radiance Field</papertitle>
              <br>
              <strong><a href="https://xusy2333.com">Shiyao Xu</a></strong>, <a href="https://lingzhili.com/">Lingzhi Li</a>,  <a href="https://scholar.google.co.uk/citations?user=ABbCaxsAAAAJ&hl=en">Li Shen</a>, <a href="https://www.wict.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em>CVPRW 2023</em>
              <br>
              <a href=".">[Project]</a>
              <a href=".">[PDF]</a>
              <a href=".">[Code]</a>
              <a href=".">[arXiv]</a>
              <br>
              <p>We propose a more efficient method, <strong>DeSRF</strong>, to stylize the radiance field, which also transfers style information to the geometry according to the input style.</p>
          </td>
      </tr>

          
          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                  <source src='images/2-teaser.mov'>
                </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Your3dEmoji: Creating Personalized Emojis via One-shot 3D-aware Cartoon Avatar Synthesis</papertitle>
              <br>
              <strong><a href="https://xusy2333.com">Shiyao Xu</a></strong>, <a href="https://lingzhili.com/">Lingzhi Li</a>,  <a href="https://scholar.google.co.uk/citations?user=ABbCaxsAAAAJ&hl=en">Li Shen</a>, <a href="https://menyifang.github.io/">Yifang Men</a>, <a href="https://www.wict.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em><strong>SIGGRAPH ASIA 2022</strong> Technical Communication</em>
              <br>
              <a href="paper/Your3dEmoji.pdf">[PDF]</a>
              <a href="https://dl.acm.org/doi/10.1145/3550340.3564220">[DOI]</a>
              <a href="https://github.com/41xu/Your3dEmoji">[Code]</a>
              <br>
              <p>We propose a novel 3D generative model to translate a real-world face image into its corresponding 3D avatar with only a single style example provided. Our model is 3D-aware in sense and also able to do attribute editing, such as smile, age, etc directly in the 3D domain.</p>
          </td>
      </tr>
          
        <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" >
                <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                  <source src='images/1-teaser.mp4'>
                </video>
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Dynamic Texture Transfer using PatchMatch and Transformers</papertitle>
              <br>
              Guo Pu*, <strong><a href="https://xusy2333.com">Shiyao Xu*</a></strong>, <a href="https://www.wict.pku.edu.cn/zlian/">Zhouhui Lian</a>
              <br>
              <em>Under Review</em>
              <p>We propose an automatically method to transfer the dynamic texture of a given video to a still image.</p>
              <details>
                <summary>Abstract</summary>
                <ol>
                How to automatically transfer the dynamic texture of a given video to the target still image is a challenging and ongoing problem. 
                In this paper, we propose to handle this task via a simple yet effective model that utilizes both PatchMatch and Transformers. 
                The key idea is to decompose the task of dynamic texture transfer into two stages, where the start frame of the target video with the desired dynamic texture is synthesized in the first stage via a distance map guided texture transfer module based on the PatchMatch algorithm. 
                Then, in the second stage, the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers equipped with VQ-VAE for processing long discrete sequences. 
                After getting all those patches, we apply a Gaussian weighted average merging strat- egy to smoothly assemble them into each frame of the target stylized video. Experimental results demonstrate the effectiveness and superiority of the proposed method in dynamic texture transfer compared to the state of the art.
                </ol>
              </details>
          </td>
      </tr>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Experiences</heading>
          <li>Reviewers for journals: IEEE Transactions on Affective Computing(TAC/TAFFC).</li>
          <li>2022.08: Serve as a Student Volunteer (virtual) in <strong>SIGGRAPH 2022</strong>.</li>
          <li>2021.08-Present: Research Intern at XR-Lab, DAMO Academy, Alibaba Inc, supervised by Lingzhi Li and Dr. Li Shen.</li>
          <li>2021.07-2021.08: Machine Learning Intern at Apple, Beijing.</li>
          <li>Spring 2021: TA of Elementary Number Theory for undergraduate students, Peking University</li>
          <li>2020.06: Join PKU-CSCL Lab led by Prof. Zhouhui Lian. Start my CV/CG jounery!</li>
          <li>2020.07: Graduate from Dalian University of Technology! üçª Thanks to Prof. Kun Lu and Prof. Shimin Shan for my undergraduate guidance!</li>
          <li>2016.09-2020.07: B.Eng. in School of Software, Dalian University of Technology. Major in Big Data and Machine Learning.</li>
        </td>
      </tr>
    </tbody></table>


      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <a href="images/emm.jpeg"><img style="width:50%;max-width:50%" alt="research" src="images/emm.jpeg" class="hoverZoomLink"></a>
        </td>
      </tr>
    </tbody></table> -->
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <p>
          I play: ‚öΩÔ∏èüèäüèª‚Äç‚ôÄÔ∏èüèÉüèª‚Äç‚ôÄÔ∏èüéæü•äüèÇüé∏
        </p>     
       </td>
    </tr>
  </tbody></table>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      <tr></tr>
        <td style="padding:20px;width:0%;vertical-align:middle">
      </td>
      <td style="padding:20px;width:100%;vertical-align:middle">
      <hr style="margin-top:0px">
        <p>Build the bridge between 2D and 3D world. Do some cool research!üòé</p>
        <p>Last modified: 13/11/2022</p>
        </td>
      </tr>




        <!-- <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
              <img src="images/DynTexture.gif" style="width:100%;max-width:100%; position: absolute;top: -5%">
              </div>
          </td>
           
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Dynamic Texture Transfer using PatchMatch and Transformers</papertitle>
              <br>
              Guo Pu*, <strong>Shiyao Xu*</strong>, Zhouhui Lian
              <br>
              <em>under-review</em>
              <p>We propose an automatically method to transfer the dynamic texture of a given video to a still image. </p>
          </td>
          </tr>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="images/emm.jpeg"><img style="width:100%;max-width:100%" alt="research" src="images/emm.jpeg" class="hoverZoomLink"></a>
            </td>
          </tr>



      </td>
    </tr>
  </table>
</body>

</html>
